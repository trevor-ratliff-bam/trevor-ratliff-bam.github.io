---
layout: post
title:  "Daily Log 2025-12-30"
date:   2025-12-30 13:05:00 -0600
category: daily
# 
# full log:
# -6:55-  up, readied for day
# -7:30-  at work and found setting in ollama to let it use intel gpu!  Also found out I should be able to use ollama models in GitHub Copilot in VS Code
# @9:00-  stand-up
# -9:10-  team review of Binh's task on 423930
# ~10:00- ??? outside to check on goats, kids said Alivar was acting odd
#   - goats were ok, dogs fought pretty bad - Kelly got bloody, may need to move where food is put so they can get to the gate without having to go past the food
# @11:00- started weekly discovery meeting - cancelled 
# -11:02- continued mucking with getting ollama models in copilot
#   - got it to work by logging into copilot with trevor-ratliff github account - then you can add the ollama models in the "manage language models" UI.  The models auto showed up for ollama, you could just show or hide the model in the model selector for chat/agent.  I later learned that you can't use a local completions model ... only chatGPT-4.1.  This is pretty much a no start for me.  So back to Continue in VS Code, and I'll have to adjust the configuration for Zed to keep the context lower (vs code used 8192kb context window and the model asked for 19gb size keeping it on the GPU)
# -12:30- worked on reviewing PR for Binh
# -1:15-  back to task 425424 ... except I got distracted by ollama again along the way
# -4:30-  off work
---

## To Do
<!--
  · = scheduled to do (Interrobang: · or &#xB7;)
  / = partially done at end of day
  x = completed at end of day 
  < = rescheduled for another day 
-->
```
< pbi 423930: 425424
x pbi 423930: 425422 PR
```

## Log
I started the day looking at settings for ollama and intel GPUs - which I finally got working with an environment variable `OLLAMA_VULKAN` set to 1.  I tried using the `GGML_VK_VISIBLE_DEVICES` variable as well, but I set it wrong, so I took it out and it seems to work fine.  I was able to run qwen3-coder:30b all on gpu from the ollama UI (30gb? size), but Zed was too greedy (45gb size with 25/75 cpu/gpu split) it also pegged my memory out at 98% used of 64gb making the whole machine sluggish.  It may have been a funny run, because it never responded and couldn't stop on its own.  

I need to play with settings without using the `OLLAMA_VULKAN` to see if VS Code can run qwen3-coder on the nvidia GPU. I still don't know if I can run both Nvidia and Intel GPUs together.  Then there is the whole NPU that I am still confused by, but if it could run embeddings

I also did the PR for 425422, and ~~worked on 425424~~ played too long with configuration of ollama.  But I did do one more PR review at the end of the day.
